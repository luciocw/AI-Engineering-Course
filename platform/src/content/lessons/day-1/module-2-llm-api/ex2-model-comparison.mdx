---
title: "Model Comparison: Choosing the Right Model"
description: "Understand different model sizes, speed vs quality vs cost tradeoffs, when to use each model, and basic benchmarking techniques."
day: "day-1"
module: "module-2-llm-api"
exercise: 2
difficulty: "beginner"
estimatedMinutes: 15
isFree: true
tags: ["llm", "api", "model-selection", "benchmarking", "haiku", "sonnet", "opus", "cost-optimization"]
---

## What You'll Learn

- The differences between small, medium, and large LLM models
- How to evaluate the speed vs quality vs cost tradeoff
- When to use each model tier in real applications
- How to run basic benchmarks to compare models

## Key Concepts

### The Model Tier System

Most LLM providers offer models in multiple sizes. Think of it like vehicles: a bicycle (small model) is cheap and fast for short trips, a car (medium model) handles most journeys well, and a truck (large model) can carry heavy loads but costs more fuel.

For the Anthropic family:

| Model | Strengths | Best For |
|-------|-----------|----------|
| Haiku | Fast, cheap | Classification, extraction, simple tasks |
| Sonnet | Balanced | Most production workloads, coding, analysis |
| Opus | Most capable | Complex reasoning, nuanced writing, research |

The same pattern applies across providers: OpenAI has GPT-4o-mini / GPT-4o / o1, Google has Flash / Pro, and so on. The principle is universal.

### Why Model Selection Matters

Choosing the right model is one of the most impactful decisions in AI engineering. Here is a real example:

```typescript
// Processing 10,000 customer support tickets per day

// With Opus (largest model):
// ~$0.075 per ticket = $750/day = $22,500/month

// With Haiku (smallest model):
// ~$0.001 per ticket = $10/day = $300/month

// If Haiku handles 95% of tickets correctly,
// you just saved $22,200/month.
```

The difference between choosing wisely and defaulting to the biggest model can be tens of thousands of dollars per month at scale.

### Comparing Models Programmatically

Here is how to run the same prompt against different models and compare:

```typescript
import Anthropic from "@anthropic-ai/sdk";

const client = new Anthropic();

interface BenchmarkResult {
  model: string;
  responseText: string;
  inputTokens: number;
  outputTokens: number;
  latencyMs: number;
  estimatedCost: number;
}

const MODEL_PRICING: Record<string, { input: number; output: number }> = {
  "claude-haiku-3-5": { input: 0.25, output: 1.25 },
  "claude-sonnet-4-20250514": { input: 3.0, output: 15.0 },
  "claude-opus-4-20250514": { input: 15.0, output: 75.0 },
};

async function benchmarkModel(
  model: string,
  prompt: string
): Promise<BenchmarkResult> {
  const startTime = Date.now();

  const response = await client.messages.create({
    model,
    max_tokens: 512,
    messages: [{ role: "user", content: prompt }],
  });

  const latencyMs = Date.now() - startTime;
  const text =
    response.content[0].type === "text" ? response.content[0].text : "";
  const pricing = MODEL_PRICING[model];
  const cost =
    (response.usage.input_tokens / 1_000_000) * pricing.input +
    (response.usage.output_tokens / 1_000_000) * pricing.output;

  return {
    model,
    responseText: text,
    inputTokens: response.usage.input_tokens,
    outputTokens: response.usage.output_tokens,
    latencyMs,
    estimatedCost: cost,
  };
}

async function compareModels(prompt: string) {
  const models = [
    "claude-haiku-3-5",
    "claude-sonnet-4-20250514",
    "claude-opus-4-20250514",
  ];

  console.log(`Prompt: "${prompt}"\n`);

  for (const model of models) {
    const result = await benchmarkModel(model, prompt);
    console.log(`--- ${result.model} ---`);
    console.log(`Latency: ${result.latencyMs}ms`);
    console.log(`Tokens: ${result.inputTokens} in / ${result.outputTokens} out`);
    console.log(`Cost: $${result.estimatedCost.toFixed(6)}`);
    console.log(`Response preview: ${result.responseText.slice(0, 150)}...`);
    console.log();
  }
}
```

### The Tradeoff Triangle

Every model selection involves balancing three factors:

```
        Quality
         /\
        /  \
       /    \
      /______\
   Speed    Cost
```

- **Quality**: How accurate, nuanced, and reliable is the output?
- **Speed**: How fast does the model respond? (Latency)
- **Cost**: How much does each call cost?

You rarely get all three. The key is knowing which matters most for your use case.

### When to Use Each Tier

**Use the smallest model (Haiku) when:**

```typescript
// Simple classification - Haiku excels at this
const response = await client.messages.create({
  model: "claude-haiku-3-5",
  max_tokens: 10,
  messages: [
    {
      role: "user",
      content: `Classify this support ticket as "billing", "technical", or "general":

      "I can't log into my account after the password reset."

      Category:`,
    },
  ],
});
// Fast, cheap, and accurate for structured tasks
```

Haiku is ideal for: classification, data extraction, simple transformations, high-volume processing, and any task where the answer space is constrained.

**Use the medium model (Sonnet) when:**

```typescript
// Analysis and content generation - Sonnet's sweet spot
const response = await client.messages.create({
  model: "claude-sonnet-4-20250514",
  max_tokens: 1024,
  messages: [
    {
      role: "user",
      content: `Analyze this code for potential bugs and suggest improvements:

      function processOrder(order) {
        const total = order.items.reduce((sum, item) => sum + item.price, 0);
        if (total > 100) order.discount = 0.1;
        sendEmail(order.customer.email, "Order confirmed");
        return saveToDatabase(order);
      }`,
    },
  ],
});
```

Sonnet is ideal for: code generation and review, content writing, data analysis, most production chatbots, and general-purpose tasks.

**Use the largest model (Opus) when:**

```typescript
// Complex multi-step reasoning - where Opus shines
const response = await client.messages.create({
  model: "claude-opus-4-20250514",
  max_tokens: 4096,
  messages: [
    {
      role: "user",
      content: `Given the following financial data for three companies,
      determine which would be the best acquisition target. Consider revenue
      growth trajectories, debt-to-equity ratios, market positioning,
      and integration risks. Provide a detailed analysis with your
      recommendation and the reasoning behind it.

      [complex financial data here...]`,
    },
  ],
});
```

Opus is ideal for: complex reasoning, research synthesis, nuanced writing, tasks requiring deep understanding, and when accuracy matters more than cost.

### Building a Model Selection Helper

In production, you might want to automatically select the model based on the task:

```typescript
type TaskComplexity = "simple" | "moderate" | "complex";

function selectModel(complexity: TaskComplexity): string {
  const modelMap: Record<TaskComplexity, string> = {
    simple: "claude-haiku-3-5",
    moderate: "claude-sonnet-4-20250514",
    complex: "claude-opus-4-20250514",
  };
  return modelMap[complexity];
}

// Use prompt length and type as heuristics
function estimateComplexity(prompt: string, taskType: string): TaskComplexity {
  if (taskType === "classification" || taskType === "extraction") {
    return "simple";
  }
  if (taskType === "analysis" || taskType === "generation") {
    return prompt.length > 2000 ? "complex" : "moderate";
  }
  return "moderate"; // Default to the balanced option
}
```

### Running a Basic Benchmark

To make informed decisions, measure what matters:

```typescript
interface QualityScore {
  accuracy: number; // 0-1: Did it get the right answer?
  completeness: number; // 0-1: Did it cover all aspects?
  relevance: number; // 0-1: Is everything relevant?
}

async function runBenchmark(
  testCases: Array<{ prompt: string; expectedOutput: string }>,
  models: string[]
) {
  const results: Record<string, BenchmarkResult[]> = {};

  for (const model of models) {
    results[model] = [];
    for (const testCase of testCases) {
      const result = await benchmarkModel(model, testCase.prompt);
      results[model].push(result);
    }
  }

  // Summarize
  for (const [model, modelResults] of Object.entries(results)) {
    const avgLatency =
      modelResults.reduce((s, r) => s + r.latencyMs, 0) / modelResults.length;
    const totalCost = modelResults.reduce((s, r) => s + r.estimatedCost, 0);

    console.log(`${model}:`);
    console.log(`  Avg latency: ${avgLatency.toFixed(0)}ms`);
    console.log(`  Total cost: $${totalCost.toFixed(6)}`);
    console.log(`  Cost per call: $${(totalCost / modelResults.length).toFixed(6)}`);
  }
}
```

## Common Mistakes

1. **Always using the biggest model**: This is the most expensive mistake in AI engineering. Most tasks do not need the most powerful model. Start with the smallest model and only scale up if quality is insufficient.

2. **Not benchmarking on YOUR data**: Generic benchmarks tell you about average performance. Your specific use case might behave differently. Always test with representative examples from your actual workload.

3. **Ignoring latency**: In user-facing applications, a 200ms response feels instant while a 5-second response feels broken. Smaller models are often 3-5x faster.

4. **Comparing only on hard cases**: If you test models on the hardest 5% of cases, the big model will always win. But if the small model handles 95% of cases correctly, you can route only the hard cases to the big model and save dramatically.

5. **Not considering output token differences**: Larger models tend to produce longer, more detailed responses. This means they cost more not just per-token but also in total token count. If you need brief answers, explicitly ask for conciseness.

## Your Task

Build a model comparison tool that:

1. Defines 3 test prompts of varying complexity (simple, moderate, complex)
2. Runs each prompt against at least two different model sizes
3. Collects latency, token usage, and estimated cost for each
4. Prints a summary table comparing the models
5. Includes a recommendation for which model to use for each prompt type

Bonus: Add a simple quality scoring mechanism where you manually rate outputs 1-5 and include that in your comparison table.

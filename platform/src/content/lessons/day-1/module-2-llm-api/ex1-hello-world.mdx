---
title: "Hello World: Your First LLM API Call"
description: "Learn what an LLM API is, make your first API call, understand request/response structure, token counting, and cost calculation."
day: "day-1"
module: "module-2-llm-api"
exercise: 1
difficulty: "beginner"
estimatedMinutes: 10
isFree: true
tags: ["llm", "api", "hello-world", "tokens", "anthropic-sdk", "getting-started"]
---

## What You'll Learn

- What an LLM API is and why it matters
- How to make your first API call using the Anthropic SDK
- The structure of API requests and responses
- How tokens work and how to estimate costs
- The difference between using an API and using a chat interface

## Key Concepts

### What Is an LLM API?

An LLM API (Large Language Model Application Programming Interface) is a way to programmatically send text to an AI model and receive a response. Instead of typing into a chat window, your code sends requests and processes responses.

Why does this matter? Because APIs let you **build products**. A chat interface is great for personal use, but an API lets you embed intelligence into applications, process thousands of requests automatically, and create entirely new user experiences.

Think of it like the difference between manually sending emails one-by-one versus writing code that sends personalized emails to your entire customer base.

### Setting Up the Anthropic SDK

First, install the SDK and set your API key:

```bash
npm install @anthropic-ai/sdk
```

```typescript
import Anthropic from "@anthropic-ai/sdk";

// The SDK reads ANTHROPIC_API_KEY from environment variables by default
const client = new Anthropic();
```

Always store your API key in environment variables, never hardcode it:

```bash
# .env file
ANTHROPIC_API_KEY=sk-ant-your-key-here
```

### Making Your First API Call

Here is the simplest possible LLM API call:

```typescript
import Anthropic from "@anthropic-ai/sdk";

const client = new Anthropic();

async function helloWorld() {
  const response = await client.messages.create({
    model: "claude-sonnet-4-20250514",
    max_tokens: 256,
    messages: [
      {
        role: "user",
        content: "Say hello and explain what you are in one sentence.",
      },
    ],
  });

  console.log(response.content[0].text);
}

helloWorld();
```

That is it. You sent text to an LLM and got text back. Everything else in this module builds on this foundation.

### Request Structure

Every API call requires these core fields:

```typescript
const response = await client.messages.create({
  // Which model to use (determines quality, speed, and cost)
  model: "claude-sonnet-4-20250514",

  // Maximum tokens the model can generate in its response
  max_tokens: 1024,

  // The conversation: an array of message objects
  messages: [
    {
      role: "user", // Who is speaking: "user" or "assistant"
      content: "Your prompt goes here",
    },
  ],
});
```

The `messages` array is the core of every request. It represents a conversation between the user and the assistant. For now, we are sending a single user message, but later you will learn to send multi-turn conversations.

### Response Structure

The API returns a structured response object:

```typescript
{
  id: "msg_01XFDUDYJgAACzvnptvVoYEL",
  type: "message",
  role: "assistant",
  model: "claude-sonnet-4-20250514",
  content: [
    {
      type: "text",
      text: "Hello! I'm an AI assistant created by Anthropic..."
    }
  ],
  stop_reason: "end_turn",
  usage: {
    input_tokens: 15,
    output_tokens: 42
  }
}
```

Key fields to understand:

- **`content`**: An array of content blocks. For text responses, access `content[0].text`.
- **`stop_reason`**: Why the model stopped generating. `"end_turn"` means it finished naturally. `"max_tokens"` means it hit the token limit (the response was cut off).
- **`usage`**: How many tokens were consumed. This is how you track costs.

### Understanding Tokens

Tokens are the fundamental unit of LLM processing. They are not words -- they are chunks of text, roughly 3-4 characters each. A word like "hello" is one token, but "extraordinary" might be three tokens.

```typescript
// Rough estimation: 1 token ~ 4 characters in English
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

// More practical: use the usage field from the response
function logUsage(response: Anthropic.Message) {
  const { input_tokens, output_tokens } = response.usage;
  console.log(`Input tokens: ${input_tokens}`);
  console.log(`Output tokens: ${output_tokens}`);
  console.log(`Total tokens: ${input_tokens + output_tokens}`);
}
```

### Cost Calculation

LLM APIs charge per token, with different rates for input and output:

```typescript
// Example pricing (always check current prices)
const PRICING = {
  "claude-sonnet-4-20250514": {
    inputPerMillion: 3.0, // $3.00 per million input tokens
    outputPerMillion: 15.0, // $15.00 per million output tokens
  },
  "claude-haiku-3-5": {
    inputPerMillion: 0.25,
    outputPerMillion: 1.25,
  },
};

function calculateCost(
  usage: { input_tokens: number; output_tokens: number },
  model: keyof typeof PRICING
): number {
  const pricing = PRICING[model];
  const inputCost = (usage.input_tokens / 1_000_000) * pricing.inputPerMillion;
  const outputCost =
    (usage.output_tokens / 1_000_000) * pricing.outputPerMillion;
  return inputCost + outputCost;
}

// After making a call:
const cost = calculateCost(response.usage, "claude-sonnet-4-20250514");
console.log(`This call cost: $${cost.toFixed(6)}`);
```

Output tokens cost significantly more than input tokens. This is important to remember when designing your prompts -- a verbose system prompt adds input cost, but asking the model to be concise saves on the more expensive output tokens.

### Putting It All Together

```typescript
import Anthropic from "@anthropic-ai/sdk";

const client = new Anthropic();

async function firstCall() {
  const startTime = Date.now();

  const response = await client.messages.create({
    model: "claude-sonnet-4-20250514",
    max_tokens: 256,
    messages: [
      {
        role: "user",
        content: "What are the three most important things to know about LLM APIs?",
      },
    ],
  });

  const elapsed = Date.now() - startTime;

  // Extract the text
  const text = response.content[0].type === "text" ? response.content[0].text : "";

  // Log everything useful
  console.log("Response:", text);
  console.log(`\nModel: ${response.model}`);
  console.log(`Stop reason: ${response.stop_reason}`);
  console.log(`Input tokens: ${response.usage.input_tokens}`);
  console.log(`Output tokens: ${response.usage.output_tokens}`);
  console.log(`Latency: ${elapsed}ms`);
}

firstCall();
```

## Common Mistakes

1. **Hardcoding API keys**: Never put your key directly in source code. Use environment variables. If you accidentally commit a key to git, rotate it immediately.

2. **Ignoring `stop_reason`**: If `stop_reason` is `"max_tokens"`, the response was truncated. Your output is incomplete and you need to either increase `max_tokens` or handle the truncation.

3. **Not setting `max_tokens` appropriately**: Setting it too low cuts off responses. Setting it too high wastes money if you are charged for reserved capacity. Start with a reasonable estimate for your use case.

4. **Assuming `content` is a string**: The `content` field is an array of content blocks. Always access `content[0].text` for text responses, and check the `type` field.

5. **Not handling errors**: API calls can fail due to rate limits, network issues, or invalid requests. Always wrap calls in try/catch:

```typescript
try {
  const response = await client.messages.create({ ... });
} catch (error) {
  if (error instanceof Anthropic.RateLimitError) {
    console.log("Rate limited. Waiting before retry...");
  } else if (error instanceof Anthropic.APIError) {
    console.log(`API error: ${error.status} - ${error.message}`);
  }
}
```

## Your Task

Create a TypeScript function called `makeFirstCall` that:

1. Makes an API call asking the model to explain a concept of your choice
2. Extracts and prints the response text
3. Logs the token usage (input and output)
4. Calculates and prints the estimated cost
5. Measures and prints the response latency in milliseconds

Bonus: Wrap the call in error handling that distinguishes between rate limit errors and other API errors.
